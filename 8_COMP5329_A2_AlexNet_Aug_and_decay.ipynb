{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8_COMP5329_A2_AlexNet_Aug_and_decay.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SWoaowRhHfFl"},"source":["# COMP5329 - Deep Learning\n","## Assignment 2 - Multilabel Classification\n","\n","*   Nirav Desai\n","*   Rebecca Chan"]},{"cell_type":"markdown","metadata":{"id":"XEGQWuj0Hjpk"},"source":["## Initial Set up\n","### Load packages"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kO7yntlyA-x1","outputId":"6775b508-f9e4-4d42-b0ed-2e29dadaafcf"},"source":["## Load libraries\n","import re, os\n","import pandas as pd\n","from io import StringIO\n","from google.colab import drive\n","\n","%matplotlib inline\n","from __future__ import print_function\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from PIL import Image\n","import itertools\n","import os\n","\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import Dataset\n","from sklearn.metrics import confusion_matrix\n","from datetime import timedelta\n","from torch import optim\n","from sklearn.metrics import classification_report\n","\n","torch.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.8.1+cu101'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"-RnfZxNWHuUD"},"source":["##### Mount google drive"]},{"cell_type":"code","metadata":{"id":"LMo_pbLhwR5P","colab":{"base_uri":"https://localhost:8080/"},"outputId":"68ebfb3a-c1ac-4853-8a51-c7f68810ecaf"},"source":["use_cuda = True if torch.cuda.is_available() else False\n","dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('We are using GPU.' if use_cuda else 'We are using CPU.')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["We are using GPU.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gk0OyKvfBXzL","outputId":"545af443-1d4c-4f52-9f46-f448dcbef5d5"},"source":["drive.mount('/content/gdrives')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrives\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HbAfmanJHw3S"},"source":["### Select Drive Path for Teammates\n","\n","**Values :** \n","- rebecca : Rebecca's drive config\n","- nirav : Nirav's drive config"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjkTH5KgLbdZ","outputId":"b2ef3d27-0e86-43ca-e398-6b926a227342"},"source":["## Input name to get correct drive path\n","name = input()\n","if name == 'rebecca':\n","  data_path = '/content/gdrives/MyDrive/COMP5329_attempt2'\n","elif name == 'nirav': \n","  data_path = '/content/gdrives/MyDrive/COMP5329/COMP5329S1A2Dataset'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["rebecca\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FlT-LdR0HzmB"},"source":["## Load Dataset Function\n","\n","**Inputs:**\n","  1. Root Directory for Images\n","  2. csv filename with annotations\n","  3. Pytorch transform object\n","\n","**Outputs:**\n","  - Training and Validation Set\n","    - 2 different method per user (Rebecca and Nirav) due to drive paths\n","    - tuple with image as a tensor and corresponding target One-Hot encoded label as tensor (image, label)\n","  - Test Set\n","    - image as a tensor"]},{"cell_type":"code","metadata":{"id":"KKTu2qEfeEid"},"source":["if name == 'nirav':\n","  class load_data(Dataset):\n","      def __init__(self, root_dir, annotation_file, transform=None):\n","          with open(root_dir+'/'+annotation_file) as train: \n","            lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in train]\n","            train_labels = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\", encoding='utf-8')\n","\n","          self.root_dir = root_dir\n","          self.annotations = train_labels\n","          self.transform = transform\n","\n","      def __len__(self):\n","          return len(self.annotations)\n","\n","      def __getitem__(self, index):\n","          img_id = self.annotations.iloc[index, 0]\n","          img = Image.open(os.path.join(self.root_dir+'/data', img_id)).convert(\"RGB\")\n","          split_label = self.annotations.iloc[index, 1].split()\n","          num_labels = list(map(int, split_label))\n","          label = torch.LongTensor(num_labels) - 1\n","\n","          # One Hot encoding Target labels\n","          one_hot_labels = torch.zeros(size=([19]), dtype=torch.float64)\n","          y_label = one_hot_labels.scatter_(dim=0, index=label, value=1.)\n","\n","          if self.transform is not None:\n","              img = self.transform(img)\n","\n","          return (img, y_label)\n","\n","elif name == 'rebecca':\n","  class load_data(Dataset):\n","      def __init__(self, root_dir, annotation_file, transform=None):\n","          with open(root_dir+'/'+annotation_file) as train: \n","            lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in train]\n","            train_labels = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\", encoding='utf-8')\n","\n","          self.root_dir = root_dir\n","          self.annotations = train_labels\n","          self.transform = transform\n","\n","      def __len__(self):\n","          return len(self.annotations)\n","\n","      def __getitem__(self, index):\n","          img_id = self.annotations.iloc[index, 0]\n","          val_temp = val_temp = int(re.search('^\\d+\\.', img_id).group(0)[:-1])\n","          if val_temp < 5000: \n","            folder_dir = 'folder_1'\n","          elif val_temp < 10000: \n","            folder_dir = 'folder_2'\n","          elif val_temp < 15000: \n","            folder_dir = 'folder_3'\n","          elif val_temp < 20000: \n","            folder_dir = 'folder_4'\n","          elif val_temp < 25000: \n","            folder_dir = 'folder_5'\n","          elif val_temp < 30000:\n","            folder_dir = 'folder_6'\n","          img = Image.open(os.path.join(self.root_dir+'/uploads/'+folder_dir, img_id)).convert(\"RGB\")\n","          split_label = self.annotations.iloc[index, 1].split()\n","          num_labels = list(map(int, split_label))\n","          label = torch.LongTensor(num_labels) - 1\n","          one_hot_labels = torch.zeros(size=([19]), dtype=torch.float64)\n","          # print(label, self.one_hot_labels.shape, label.shape)\n","          y_label = one_hot_labels.scatter_(dim=0, index=label, value=1.)\n","          # y_label = torch.tensor(float(self.annotations.iloc[index, 1]))\n","\n","          if self.transform is not None:\n","              img = self.transform(img)\n","\n","          return (img, y_label)\n","\n","if name == 'nirav':\n","  class load_test_data(Dataset):\n","      def __init__(self, root_dir, annotation_file, transform=None): \n","          with open(root_dir + '/' + annotation_file) as test: \n","            lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in test]\n","            test_labels = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\", encoding='utf-8')    \n","\n","          self.root_dir = root_dir\n","          self.annotations = test_labels\n","          self.transform = transform\n","        \n","      def __len__(self):\n","          return len(self.annotations)\n","\n","      def __getitem__(self, index):\n","          img_id = self.annotations.iloc[index,0]\n","          img = Image.open(os.path.join(self.root_dir+'/data', img_id)).convert(\"RGB\")\n","\n","          if self.transform is not None: \n","              img = self.transform(img)\n","\n","          return (img)\n","\n","elif name == 'rebecca':\n","  class load_test_data(Dataset):\n","      def __init__(self, root_dir, annotation_file, transform=None): \n","          with open(root_dir + '/' + annotation_file) as test: \n","            lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in test]\n","            test_labels = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\", encoding='utf-8')    \n","\n","          self.root_dir = root_dir\n","          self.annotations = test_labels\n","          self.transform = transform\n","        \n","      def __len__(self):\n","          return len(self.annotations)\n","\n","      def __getitem__(self, index):\n","          img_id = self.annotations.iloc[index,0]\n","          img = Image.open(os.path.join(self.root_dir+'/test_data/test_set', img_id)).convert(\"RGB\")\n","\n","          if self.transform is not None: \n","              img = self.transform(img)\n","\n","          return (img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_OJ2TRuH2lw"},"source":["## Image Pre-processing and Augmentation\n","\n","- Augmentation Techniues such as:\n","  - Crop\n","  - Resize\n","  - Flipping (Horizontal and Vertical)\n","  - Rotation\n","  - Brightness Changes\n","- Normalize\n","- Load raw data and split into Training and Validation\n","- Create pytorch DataLoader instance for Train, Validation and Test set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUJFyMNzUkNg","outputId":"125b519a-46dc-4d7f-8dc1-c5baebb71eb8"},"source":["# https://stackoverflow.com/questions/56774582/adding-custom-labels-to-pytorch-dataloader-dataset-does-not-work-for-custom-data (for adding labels)\n","# https://medium.com/analytics-vidhya/implementing-cnn-in-pytorch-with-custom-dataset-and-transfer-learning-1864daac14cc (maybe better?)\n","# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html try this\n","\n","# DataLoader Parameters\n","batch_size = 32\n","num_workers = 2\n","pin_memory = True\n","shuffle=True\n","\n","kwargs = {'num_workers': num_workers, 'pin_memory': pin_memory} if use_cuda else {}\n","kwargs['batch_size'] = batch_size  # Batch size used during training.\n","\n","# Transform for data pre-processing\n","transform = transforms.Compose([transforms.Resize(240), \n","                                transforms.CenterCrop(224),\n","                                transforms.RandomHorizontalFlip(),\n","                                transforms.RandomVerticalFlip(),\n","                                transforms.RandomRotation(degrees=[-30, 30]),\n","                                transforms.ColorJitter(brightness=0.4),\n","                                transforms.ToTensor(), \n","                                transforms.Normalize((0.4914, 0.48216, 0.44653), (0.24703, 0.24349, 0.26159))\n","                                ])\n","\n","test_transform = transforms.Compose([transforms.Resize(240), \n","                                transforms.CenterCrop(224),\n","                                transforms.ToTensor(), \n","                                transforms.Normalize((0.4914, 0.48216, 0.44653), (0.24703, 0.24349, 0.26159))\n","                                ])\n","\n","raw_dataset = load_data(data_path,\"train.csv\",transform=transform)\n","test_dataset = load_test_data(data_path, \"test.csv\", transform=test_transform)\n","\n","# Train - Validation split\n","total_raw_data = len(raw_dataset)\n","train_valid_split = 0.2\n","valid_record_size = int(train_valid_split * total_raw_data)\n","train_record_size = int(total_raw_data - valid_record_size)\n","print(train_record_size, valid_record_size)\n","\n","train_set, validation_set = torch.utils.data.random_split(raw_dataset, [train_record_size,valid_record_size])\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_set, \n","                                           shuffle=shuffle, \n","                                           batch_size=batch_size,\n","                                           num_workers=num_workers,\n","                                           pin_memory=pin_memory)\n","\n","validation_loader = torch.utils.data.DataLoader(dataset=validation_set, \n","                                                shuffle=shuffle, \n","                                                batch_size=batch_size,\n","                                                num_workers=num_workers, \n","                                                pin_memory=pin_memory)\n","\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","                                          shuffle = False, \n","                                          batch_size = None\n","                                          )\n","print('Dataloaders initialized.')\n","\n","images, labels = next(iter(train_loader))\n","print('Shape of an image batch:', images.size())\n","\n","# Should be approx. 40,000 images (30,000 for training, 10,000 for testing)\n","print('No. images loaded: ',len(train_loader)*batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["23997 5999\n","Dataloaders initialized.\n","Shape of an image batch: torch.Size([32, 3, 224, 224])\n","No. images loaded:  24000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fc7y8sYSIAt7"},"source":["## Exploratory Dataset Analysis\n","\n","#### Training Labels"]},{"cell_type":"code","metadata":{"id":"iIKsJhJnRiZq"},"source":["training_indices = raw_dataset.annotations['Labels'][train_set.indices]\n","ti_split = training_indices.str.split()\n","t_counts = ti_split.explode().value_counts()\n","t_counts.index = t_counts.index.astype(int)\n","t_cnt = t_counts.sort_index()\n","print(t_cnt)\n","t_cnt = t_cnt.to_numpy()\n","total_cnt = np.insert(t_cnt,11,0) # for missing label 12, inserting 0\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hq-4MbGxIGGc"},"source":["#### Training Label Distribution"]},{"cell_type":"code","metadata":{"id":"ZlyKIc0dRkEN"},"source":["%matplotlib inline\n","class_labels = ['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6', 'Class 7', 'Class 8', 'Class 9', 'Class 10', 'Class 11', 'Class 12', 'Class 13', 'Class 14', 'Class 15', \n","'Class 16', 'Class 17', 'Class 18','Class 19']\n","plt.rcParams['figure.figsize'] = [8, 8]\n","plt.title('Training Data Label Distribution')\n","plt.ylabel('Count of Samples')\n","plt.xlabel('Training Label')\n","plt.xticks(np.arange(0,19,1), class_labels, rotation=45)\n","plt.bar(np.arange(len(total_cnt)),total_cnt, width=0.7)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fOkdqczRy8fS"},"source":["### Functions and definitions necessary for evaluation"]},{"cell_type":"code","metadata":{"id":"bA4wcRYCLBd7"},"source":["def accuracy(predicted, original):  \n","    \"\"\"\n","    Calculates the number of labels correctly predicted for each image\n","    \"\"\"\n","    return torch.round(predicted).eq(original).sum().numpy()/len(original)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9_jPzLiK_V4"},"source":["## Create list of class names for matching in the evaluation and prediction stage\n","class_names = np.array(list(range(1, 20)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MWMguCBdEQBx"},"source":["## Modelling stage:\n","### Implement AlexNet"]},{"cell_type":"code","metadata":{"id":"0MV1nbJnwkz_"},"source":["class AlexNet(nn.Module):\n","\n","    def __init__(self, num_classes=19, stem_stride=4):\n","        # invoke super class initialisation method\n","        super(AlexNet, self).__init__()\n","        \n","        # define the CNN:\n","        # 1. define feature extraction layers\n","        self.features = nn.Sequential(\n","            # conv-relu-pooling\n","            nn.Conv2d(3, 64, kernel_size=11, stride=stem_stride, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            # conv-relu-pooling\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            # conv-relu-conv-relu-conv-relu-pooling\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        # 2. define average pooling layer\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        # 3. define fully connected layers\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(),                   # use dropout\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),                   # use dropout\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, num_classes),\n","            nn.Sigmoid(),\n","        )\n","\n","    # define forward-propagation\n","    def forward(self, x):\n","        # feature extraction\n","        x = self.features(x)\n","        # adaptive pooling layer\n","        x = self.avgpool(x)\n","        # flatten feature map\n","        x = torch.flatten(x, 1)\n","        # classification\n","        x = self.classifier(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s6Z5IKX-IR3H"},"source":["### Model Training with hypermarameters"]},{"cell_type":"code","metadata":{"id":"ihSYe6DYgQhc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e73a749f-6e2b-4a65-b464-1de83637ebca"},"source":["## Initialise and train an alexnet model\n","model_alexnet = AlexNet().to(dev)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model_alexnet.parameters(), lr = 3e-4, weight_decay = 0.01)\n","\n","train_loss_tracker = []\n","train_accuracy_tracker = []\n","epoch_loss = []\n","\n","epochs = 25\n","for epoch in range(epochs):\n","  print(f'Epoch no. {epoch + 1}')\n","\n","  start = time.time()\n","  ep_loss = 0.0\n","  ep_acc = 0.0\n","\n","  for i, data in enumerate(train_loader, 0):\n","    inputs, labels = data\n","\n","    inputs = inputs.to(dev)\n","    labels = labels.to(dev)\n","\n","    optimizer.zero_grad()\n","\n","    outputs = model_alexnet(inputs)\n","    loss = criterion(outputs.float(), labels.float())\n","    ep_loss += loss\n","\n","    acc = accuracy(outputs.to('cpu'), labels.to('cpu'))\n","    ep_acc += acc\n","    loss.backward()\n","    optimizer.step()\n","  \n","  train_loss_tracker.append(ep_loss/(i+1))\n","  train_accuracy_tracker.append(ep_acc/(i+1))\n","  print(f'Loss: {ep_loss/(i+1) :.3f}, Accuracy: {ep_acc/(i+1) :.3f}')\n","  print(f'time taken to run: {time.time() - start}')\n","  epoch_loss.append(round(ep_loss.item()/(i+1),2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch no. 1\n","Loss: 0.205, Accuracy: 17.955\n","time taken to run: 5501.818663358688\n","Epoch no. 2\n","Loss: 0.198, Accuracy: 17.968\n","time taken to run: 85.37167525291443\n","Epoch no. 3\n","Loss: 0.197, Accuracy: 17.968\n","time taken to run: 84.40736436843872\n","Epoch no. 4\n","Loss: 0.197, Accuracy: 17.968\n","time taken to run: 83.40253758430481\n","Epoch no. 5\n","Loss: 0.196, Accuracy: 17.968\n","time taken to run: 82.6618423461914\n","Epoch no. 6\n","Loss: 0.196, Accuracy: 17.968\n","time taken to run: 81.85242629051208\n","Epoch no. 7\n","Loss: 0.196, Accuracy: 17.968\n","time taken to run: 84.27642512321472\n","Epoch no. 8\n","Loss: 0.196, Accuracy: 17.968\n","time taken to run: 84.80474400520325\n","Epoch no. 9\n","Loss: 0.195, Accuracy: 17.968\n","time taken to run: 85.07283163070679\n","Epoch no. 10\n","Loss: 0.196, Accuracy: 17.968\n","time taken to run: 83.63415455818176\n","Epoch no. 11\n","Loss: 0.196, Accuracy: 17.968\n","time taken to run: 79.14959454536438\n","Epoch no. 12\n","Loss: 0.195, Accuracy: 17.968\n","time taken to run: 78.74146771430969\n","Epoch no. 13\n","Loss: 0.195, Accuracy: 17.968\n","time taken to run: 82.21905183792114\n","Epoch no. 14\n","Loss: 0.195, Accuracy: 17.968\n","time taken to run: 78.54730534553528\n","Epoch no. 15\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 77.92510676383972\n","Epoch no. 16\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 77.7595796585083\n","Epoch no. 17\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 78.9074318408966\n","Epoch no. 18\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 77.56822299957275\n","Epoch no. 19\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 77.59141755104065\n","Epoch no. 20\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 77.92865419387817\n","Epoch no. 21\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 77.86179995536804\n","Epoch no. 22\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 78.19854617118835\n","Epoch no. 23\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 77.7475278377533\n","Epoch no. 24\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 77.55727910995483\n","Epoch no. 25\n","Loss: 0.194, Accuracy: 17.968\n","time taken to run: 78.55907559394836\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qTbq5kG9wiMR"},"source":["torch.save(model_alexnet.state_dict(), data_path+'/trained_alexnet_with_aug_wd.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOxMrLmOPiPN"},"source":["plt.figure(figsize=(15,4))\n","plt.title('Training Loss per epoch')\n","plt.ylabel('Training Loss')\n","plt.xlabel('Epochs')\n","plt.plot(epoch_loss)\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ebeGoXfRxEuC","outputId":"ec956987-7661-45dd-e5e4-24b7e087bc2a"},"source":["model_alexnet = AlexNet()\n","model_alexnet.load_state_dict(torch.load(data_path+'/trained_alexnet_with_aug_wd.pth'))\n","model_alexnet.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AlexNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n","    (1): ReLU(inplace=True)\n","    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (4): ReLU(inplace=True)\n","    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace=True)\n","    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=9216, out_features=4096, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace=True)\n","    (6): Linear(in_features=4096, out_features=19, bias=True)\n","    (7): Sigmoid()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63sM_Kbp0le3","outputId":"258536e6-7ce8-4de5-b800-e63754b4d2ba"},"source":["criterion = nn.BCELoss()\n","# model_alexnet.to(dev)\n","model_alexnet.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AlexNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n","    (1): ReLU(inplace=True)\n","    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (4): ReLU(inplace=True)\n","    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace=True)\n","    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=9216, out_features=4096, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace=True)\n","    (6): Linear(in_features=4096, out_features=19, bias=True)\n","    (7): Sigmoid()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"itM6C3FoIY0Q"},"source":["### Model Validation on validation set data"]},{"cell_type":"code","metadata":{"id":"OMO0YY6WEkJE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"108dbae3-2c0d-4b23-9165-41cc62b209cf"},"source":["model_alexnet.cpu()\n","model_alexnet.eval()\n","\n","pred_results = np.array(range(1,20)).reshape(1,19)\n","target_results = np.array(range(1,20)).reshape(1,19)\n","\n","validation_loss = 0.0\n","validation_accuracy = 0.0\n","\n","for step, (inputs, label) in enumerate(validation_loader):\n","  print(step)\n","  inputs = torch.Tensor.cpu(inputs)#.to(dev)\n","  label = torch.Tensor.cpu(label)#.to(dev)\n","\n","  # Make predictions\n","  y_pred = model_alexnet(inputs)\n","  preds_rounded = torch.round(y_pred) #anything greater than prob > 0.5 converted to predicted label\n","  preds_np = torch.Tensor.cpu(preds_rounded).detach().numpy()\n","  targets_np = torch.Tensor.cpu(label).detach().numpy()\n","\n","  # Store predictions\n","  pred_results = np.concatenate((pred_results, preds_np), axis=0)\n","  target_results = np.concatenate((target_results, targets_np), axis=0)\n","\n","  acc = accuracy(y_pred.cpu(), label.cpu())\n","  validation_loss += criterion(y_pred.float(), label.float())\n","  validation_accuracy += acc\n","\n","average_validation_loss = validation_loss/(len(validation_loader))\n","average_validation_accuracy = validation_accuracy/(len(validation_loader))\n","\n","print(f'Average validation loss: {average_validation_loss}')\n","print(f'Average validation accuracy: {average_validation_accuracy}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","Average validation loss: 0.1897120624780655\n","Average validation accuracy: 17.971409574468087\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SrGvQqMqIla5"},"source":["### Model evaluation\n","- Classification Report\n","- Confusion Matrix for all classes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-Ja98sFr6TP","outputId":"752b18a4-66b8-4e31-facd-a2dbc70df0c4"},"source":["classes = [str(i).zfill(2) for i in range(1,20)]\n","print(class_names, classes)\n","print(classification_report(target_results[1:], pred_results[1:], target_names=classes , zero_division=1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19] ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']\n","              precision    recall  f1-score   support\n","\n","          01       0.76      1.00      0.86      4555\n","          02       1.00      0.00      0.00       233\n","          03       1.00      0.00      0.00       884\n","          04       1.00      0.00      0.00       249\n","          05       1.00      0.00      0.00       231\n","          06       1.00      0.00      0.00       277\n","          07       1.00      0.00      0.00       238\n","          08       1.00      0.00      0.00       439\n","          09       1.00      0.00      0.00       194\n","          10       1.00      0.00      0.00       310\n","          11       1.00      0.00      0.00       111\n","          12       1.00      1.00      1.00         0\n","          13       1.00      0.00      0.00       101\n","          14       1.00      0.00      0.00        53\n","          15       1.00      0.00      0.00       397\n","          16       1.00      0.00      0.00       213\n","          17       1.00      0.00      0.00       269\n","          18       1.00      0.00      0.00       317\n","          19       1.00      0.00      0.00       211\n","\n","   micro avg       0.76      0.49      0.60      9282\n","   macro avg       0.99      0.11      0.10      9282\n","weighted avg       0.88      0.49      0.42      9282\n"," samples avg       0.76      0.60      0.64      9282\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sFvha5l_LB7K","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c9b4529-5a58-40f0-d39c-3b072e41af87"},"source":["from sklearn.metrics import multilabel_confusion_matrix, coverage_error\n","coverage_error(target_results[1:], pred_results[1:])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10.385564260710119"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0mqvWzgSFjLa","outputId":"dfc83e2c-fa34-4018-f5ce-d9cc00baff28"},"source":["# multilabel_confusion_matrix(target_results[1:], pred_results[1:])\n","\n","multi_matrix = multilabel_confusion_matrix(target_results[1:], pred_results[1:])\n","\n","labels = [\"\".join(\"c\" + str(i)) for i in range(1, 20)]\n","\n","def display_heatmap(confusion_matrix, axes, class_labels, class_names, fontsize=16):\n","\n","    df_cm = pd.DataFrame(\n","        confusion_matrix, index=class_names, columns=class_names,\n","    )\n","    heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes)\n","    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n","    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n","    axes.set_ylabel('True label')\n","    axes.set_xlabel('Predicted label')\n","    axes.set_title(\"Confusion Matrix for - \" + class_labels)\n","\n","\n","fig, ax = plt.subplots(4, 5, figsize=(15, 10))\n","for axes, cfs_matrix, label in zip(ax.flatten(), multi_matrix, labels):\n","        display_heatmap(cfs_matrix, axes, label, [\"N\", \"Y\"])\n","    \n","fig.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[   0, 1444],\n","        [   0, 4555]],\n","\n","       [[5766,    0],\n","        [ 233,    0]],\n","\n","       [[5115,    0],\n","        [ 884,    0]],\n","\n","       [[5750,    0],\n","        [ 249,    0]],\n","\n","       [[5768,    0],\n","        [ 231,    0]],\n","\n","       [[5722,    0],\n","        [ 277,    0]],\n","\n","       [[5761,    0],\n","        [ 238,    0]],\n","\n","       [[5560,    0],\n","        [ 439,    0]],\n","\n","       [[5805,    0],\n","        [ 194,    0]],\n","\n","       [[5689,    0],\n","        [ 310,    0]],\n","\n","       [[5888,    0],\n","        [ 111,    0]],\n","\n","       [[5999,    0],\n","        [   0,    0]],\n","\n","       [[5898,    0],\n","        [ 101,    0]],\n","\n","       [[5946,    0],\n","        [  53,    0]],\n","\n","       [[5602,    0],\n","        [ 397,    0]],\n","\n","       [[5786,    0],\n","        [ 213,    0]],\n","\n","       [[5730,    0],\n","        [ 269,    0]],\n","\n","       [[5682,    0],\n","        [ 317,    0]],\n","\n","       [[5788,    0],\n","        [ 211,    0]]])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"mIk6yJUmIpR-"},"source":["## Test Prediction\n","- Initialize test loader\n","- Run and save predictions"]},{"cell_type":"code","metadata":{"id":"iH6Nndcjt-Tc","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"bc8680ca-b08e-4949-e378-23412ef0f6d8"},"source":["## Make test predictions\n","model_alexnet.eval()\n","pred_results_test = np.array(range(1,20)).reshape(1,19)\n","images = []\n","predictions = []\n","\n","for step, (inputs) in enumerate(test_loader):\n","    if step % 100 == 0:\n","      print(step)\n","    images.append(test_dataset.annotations.ImageID[step])\n","    inputs = torch.Tensor.cpu(inputs)#.to(dev)\n","    inputs = inputs.unsqueeze(0)\n","\n","    y_pred = model_alexnet(inputs)\n","\n","    preds_rounded = torch.round(y_pred) #anything greater than prob > 0.5 converted to predicted label\n","    preds_np = torch.Tensor.cpu(preds_rounded).detach().numpy()\n","\n","    preds_classes = class_names[preds_np[0] == 1.0]\n","    preds_string = [str(int) for int in preds_classes]\n","    \n","    predictions.append(' '.join(preds_string))\n","    pred_results_test = np.concatenate((pred_results_test, preds_np), axis=0)\n","\n","prediction_upload = pd.DataFrame(zip(images, predictions), columns = ['ImageID', 'Labels'])\n","prediction_upload.to_csv(data_path + '/alexnet_aug_wd_test_predictions.csv', index = False)\n","prediction_upload.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n","1600\n","1700\n","1800\n","1900\n","2000\n","2100\n","2200\n","2300\n","2400\n","2500\n","2600\n","2700\n","2800\n","2900\n","3000\n","3100\n","3200\n","3300\n","3400\n","3500\n","3600\n","3700\n","3800\n","3900\n","4000\n","4100\n","4200\n","4300\n","4400\n","4500\n","4600\n","4700\n","4800\n","4900\n","5000\n","5100\n","5200\n","5300\n","5400\n","5500\n","5600\n","5700\n","5800\n","5900\n","6000\n","6100\n","6200\n","6300\n","6400\n","6500\n","6600\n","6700\n","6800\n","6900\n","7000\n","7100\n","7200\n","7300\n","7400\n","7500\n","7600\n","7700\n","7800\n","7900\n","8000\n","8100\n","8200\n","8300\n","8400\n","8500\n","8600\n","8700\n","8800\n","8900\n","9000\n","9100\n","9200\n","9300\n","9400\n","9500\n","9600\n","9700\n","9800\n","9900\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ImageID</th>\n","      <th>Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>30000.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>30001.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30002.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>30003.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>30004.jpg</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     ImageID Labels\n","0  30000.jpg      1\n","1  30001.jpg      1\n","2  30002.jpg      1\n","3  30003.jpg      1\n","4  30004.jpg      1"]},"metadata":{"tags":[]},"execution_count":12}]}]}